{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "data = pd.read_csv(\"telco_customer_churn.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Total Charges to a numerical data type\n",
    "data.TotalCharges = pd.to_numeric(data.TotalCharges, errors=\"coerce\")\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are eleven missing values in TotalCharges for some of the customers with zero tenure. We can impute these values with zero as these customers probably haven't paied any bills yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with 0\n",
    "data[\"TotalCharges\"] = data[\"TotalCharges\"].replace(\" \", 0).astype(\"float32\")\n",
    "# data[\"TotalCharges\"] = data[\"TotalCharges\"].replace(\" \", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(data[\"tenure\"], hist=True, kde=False, hist_kws={\"edgecolor\":\"black\"}, kde_kws={\"linewidth\": 6})\n",
    "ax.set_ylabel(\"Customers\")\n",
    "ax.set_xlabel(\"Tenure (months)\")\n",
    "ax.set_title(\"Number of customers by their tenure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target feature counts\n",
    "data[\"Churn\"].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating categorical columns for tenure feature\n",
    "def tenure_lab(data):\n",
    "    if data[\"tenure\"] <= 12:\n",
    "        return \"Tenure_0-12\"\n",
    "    elif (data[\"tenure\"] > 12) & (data[\"tenure\"] <= 24):\n",
    "        return \"Tenure_12-24\"\n",
    "    elif (data[\"tenure\"] > 24) & (data[\"tenure\"] <= 48):\n",
    "        return \"Tenure_24-48\"\n",
    "    elif (data[\"tenure\"] > 48) & (data[\"tenure\"] <= 60):\n",
    "        return \"Tenure_48-60\"\n",
    "    elif data[\"tenure\"] > 60:\n",
    "        return \"Tenure_gt_60\"\n",
    "data[\"tenure_group\"] = data.apply(lambda data:tenure_lab(data), axis=1)\n",
    "\n",
    "# Customer attrition in tenure groups\n",
    "sns.catplot(x=\"tenure_group\", hue=\"Churn\", kind=\"count\", data=data, aspect=1.6, height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Replace binary values to numeric values\n",
    "data[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].replace({1:\"Yes\", 0:\"No\"})\n",
    "\n",
    "# Converting the predictor variable to binary numeric variable\n",
    "df = data.iloc[:,1:]\n",
    "df[\"Churn\"].replace(to_replace=\"Yes\", value=1, inplace=True)\n",
    "df[\"Churn\"].replace(to_replace=\"No\", value=0, inplace=True)\n",
    "df_dummies = pd.get_dummies(df)\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of Churn with other features\n",
    "plt.figure(figsize=(15,8))\n",
    "df_dummies.corr()[\"Churn\"].sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contracts information\n",
    "ax = data[\"Contract\"].value_counts().plot(kind=\"bar\", rot=0, width=0.5)\n",
    "ax.set_ylabel(\"Number of Customers\")\n",
    "ax.set_title(\"Number of customers by contract type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonthlyCharges and TotalCharges information\n",
    "data[[\"MonthlyCharges\", \"TotalCharges\"]].plot.scatter(x=\"MonthlyCharges\", y=\"TotalCharges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn rates of customers\n",
    "df = data[\"Churn\"].value_counts() * 100.0 / len(data)\n",
    "ax = df.plot(kind=\"bar\", stacked=True, rot=0, figsize=(9,7))\n",
    "ax.set_ylabel(\"% Customers\", fontsize=13)\n",
    "ax.set_xlabel(\"Churn\", fontsize=13)\n",
    "ax.set_title(\"Churn Rate\", fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn by tenure\n",
    "sns.boxplot(x=data.Churn, y=data.tenure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn by contract type\n",
    "df = data.groupby([\"Contract\", \"Churn\"]).size().unstack()\n",
    "df1 = df.T * 100.0 / df.T.sum()\n",
    "ax = df1.T.plot(kind=\"bar\", stacked=True, rot=0, figsize=(10,6))\n",
    "ax.legend(loc=\"best\", prop={\"size\":13}, title=\"Churn\")\n",
    "ax.set_title(\"Churn by contract type\", size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn by seniority\n",
    "df = data.groupby([\"SeniorCitizen\", \"Churn\"]).size().unstack()\n",
    "df1 = df.T * 100.0 / df.T.sum()\n",
    "ax = df1.T.plot(kind=\"bar\", stacked=True, rot=0, figsize=(9,6))\n",
    "ax.legend(loc=\"best\", prop={\"size\":13}, title=\"Churn\")\n",
    "ax.set_title(\"Churn by seniority level\", size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Scaling all variables to a range of 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "y = df_dummies[\"Churn\"].values\n",
    "X = df_dummies.drop(columns = [\"Churn\"])\n",
    "features = X.columns.values\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "X.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building (Logistic Regression)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=111)\n",
    "model1 = LogisticRegression()\n",
    "result = model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Predictions\n",
    "prediction_test = model1.predict(X_test)\n",
    "print(\"Classification report: \\n\", classification_report(y_test, prediction_test))\n",
    "print(\"Accuracy score: {:.5f}\\n\".format(accuracy_score(y_test, prediction_test)))\n",
    "print(\"f1_score: {:.5f}\\n\".format(f1_score(y_test, prediction_test)))\n",
    "print(\"Cohen's kappa score: {:.5f}\\n\".format(cohen_kappa_score(y_test, prediction_test)))\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "predictions = model1.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recallscore = recall_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "f1score = f1_score(y_test, predictions) \n",
    "kappa_metric = cohen_kappa_score(y_test, predictions)\n",
    "\n",
    "df = pd.DataFrame({\"Accuracy_score\": [accuracy],\n",
    "                   \"Recall_score\": [recallscore],\n",
    "                   \"Precision\": [precision],\n",
    "                   \"f1_score\": [f1score],\n",
    "                   \"Area_under_curve\": [roc_auc],\n",
    "                   \"Kappa_metric\": [kappa_metric]})\n",
    "\n",
    "model_performance = pd.concat([df], axis = 0).reset_index()\n",
    "model_performance = model_performance.drop(columns = \"index\", axis=1)\n",
    "table = ff.create_table(np.round(model_performance, 5))\n",
    "py.iplot(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 10 Folds Cross Validation\n",
    "clf_score = cross_val_score(model1, X_train, y_train, cv=10)\n",
    "print(clf_score)\n",
    "clf_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the weights of all variables\n",
    "weights = pd.Series(model1.coef_[0], index=X.columns.values)\n",
    "print(weights.sort_values(ascending=False)[:10].plot(kind=\"bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "# Artificial neural networks model building\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=110)\n",
    "model2 = Sequential()\n",
    "input_shape = X_train.shape[1]\n",
    "model2.add(layers.Dense(1024, input_shape=(input_shape,), activation=\"relu\"))\n",
    "\n",
    "# Dropout for avoiding overfitting\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(layers.Dense(1024, activation=\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_keras = model2.fit(X_train, y_train, epochs=100, verbose=True, \n",
    "                       validation_data=(X_test, y_test), batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training score: {:.5f}\".format(accuracy[0]))\n",
    "print(\"Training accuracy: {:.5f}\\n\".format(accuracy[1]))\n",
    "\n",
    "accuracy = model2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing score: {:.5f}\".format(accuracy[0]))\n",
    "print(\"Testing accuracy: {:.5f}\\n\".format(accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
